"""
æ”¹è¿›æ¨ç†è„šæœ¬ - ä½¿ç”¨éšæœºé‡‡æ ·ç­–ç•¥
"""

import json
from pathlib import Path

import numpy as np
import torch
from torch.distributions import Categorical

from src.app_config import ModelConfig
from src.envs.mario import MarioEnvConfig, MarioVectorEnvConfig, create_vector_env
from src.models import MarioActorCritic


def run_episode_with_sampling():
    # é…ç½®
    checkpoint_path = Path("trained_models/run01/a3c_world1_stage1_0008000.pt")
    metadata_path = checkpoint_path.with_suffix(".json")

    # è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½å…ƒæ•°æ®
    with open(metadata_path, "r", encoding="utf-8") as f:
        metadata = json.load(f)

    print(f"åŠ è½½æ¨¡å‹: {checkpoint_path.name}")
    print(
        f"World {metadata['world']}-{metadata['stage']} | åŠ¨ä½œç±»å‹: {metadata['action_type']}"
    )

    # åˆ›å»ºæ¨¡å‹
    model_cfg = ModelConfig(**metadata["model"])
    model = MarioActorCritic(model_cfg).to(device)

    # åŠ è½½æƒé‡
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint["model"])
    model.eval()

    # åˆ›å»ºç¯å¢ƒ
    env_cfg = MarioEnvConfig(
        world=metadata["world"],
        stage=metadata["stage"],
        action_type=metadata["action_type"],
        frame_skip=metadata["frame_skip"],
        frame_stack=metadata["frame_stack"],
    )

    vector_cfg = MarioVectorEnvConfig(
        num_envs=1,
        asynchronous=False,
        stage_schedule=tuple([(metadata["world"], metadata["stage"])]),
        random_start_stage=False,
        base_seed=42,
        env=env_cfg,
    )

    env = create_vector_env(vector_cfg)

    try:
        results = []

        # è¿è¡Œå¤šä¸ªepisodesï¼Œæ¯”è¾ƒä¸åŒç­–ç•¥
        strategies = [
            ("è´ªå©ªç­–ç•¥", "greedy"),
            ("éšæœºé‡‡æ ·", "sample"),
            ("æ¸©åº¦é‡‡æ ·", "temperature"),
        ]

        for strategy_name, strategy_type in strategies:
            print(f"\n=== {strategy_name} ===")

            obs_np, info = env.reset()
            obs = torch.as_tensor(obs_np, dtype=torch.float32, device=device)
            hidden_state, cell_state = model.initial_state(1, device)

            total_reward = 0.0
            steps = 0
            max_x_pos = 0
            positions = []

            for step in range(2000):  # é™åˆ¶2000æ­¥
                with torch.no_grad():
                    output = model(obs, hidden_state, cell_state)
                    logits = output.logits

                    # ä¸åŒç­–ç•¥é€‰æ‹©åŠ¨ä½œ
                    if strategy_type == "greedy":
                        action = torch.argmax(logits, dim=-1)
                    elif strategy_type == "sample":
                        dist = Categorical(logits=logits)
                        action = dist.sample()
                    else:  # temperature sampling
                        # é™ä½æ¸©åº¦ä½¿ç­–ç•¥æ›´éšæœº
                        temperature = 2.0
                        dist = Categorical(logits=logits / temperature)
                        action = dist.sample()

                action_np = action.cpu().numpy()
                obs_np, reward_np, terminated, truncated, info_dict = env.step(
                    action_np
                )

                reward = float(reward_np[0])
                total_reward += reward
                steps += 1

                # æ›´æ–°çŠ¶æ€
                obs = torch.as_tensor(obs_np, dtype=torch.float32, device=device)
                if output.hidden_state is not None:
                    hidden_state = output.hidden_state
                    if output.cell_state is not None:
                        cell_state = output.cell_state

                # è®°å½•ä½ç½®ä¿¡æ¯
                if info_dict and len(info_dict) > 0:
                    if isinstance(info_dict, list):
                        game_info = info_dict[0] if len(info_dict) > 0 else {}
                    else:
                        game_info = info_dict

                    x_pos = game_info.get("x_pos", 0)
                    if x_pos > 0:
                        positions.append(x_pos)
                        max_x_pos = max(max_x_pos, x_pos)

                    # æ£€æŸ¥é€šå…³
                    if game_info.get("flag_get", False):
                        print(
                            f"ğŸ é€šå…³æˆåŠŸ! æ­¥æ•°: {steps}, å¥–åŠ±: {total_reward:.1f}, ä½ç½®: {x_pos}"
                        )
                        break

                # æ¯200æ­¥è¾“å‡ºè¿›åº¦
                if (step + 1) % 200 == 0:
                    print(
                        f"  æ­¥æ•°: {step+1:4d} | æœ€è¿œä½ç½®: {max_x_pos:4d} | ç´¯è®¡å¥–åŠ±: {total_reward:7.1f}"
                    )

                if terminated[0] or truncated[0]:
                    break

            # è®¡ç®—ç§»åŠ¨ç»Ÿè®¡
            if positions:
                avg_pos = np.mean(positions)
                pos_progress = max_x_pos - (positions[0] if positions else 0)
            else:
                avg_pos = 0
                pos_progress = 0

            result = {
                "strategy": strategy_name,
                "reward": total_reward,
                "steps": steps,
                "max_x_pos": max_x_pos,
                "avg_x_pos": avg_pos,
                "progress": pos_progress,
                "success": max_x_pos > 100,  # ç®€å•æˆåŠŸåˆ¤æ–­
            }
            results.append(result)

            print(
                f"ç»“æœ - å¥–åŠ±: {total_reward:7.1f} | æœ€è¿œ: {max_x_pos:4d} | æ­¥æ•°: {steps:4d}"
            )

        # æ€»ç»“å¯¹æ¯”
        print("\n=== ç­–ç•¥å¯¹æ¯”æ€»ç»“ ===")
        for result in results:
            success_mark = "âœ“" if result["success"] else "âœ—"
            print(
                f"{success_mark} {result['strategy']:8s}: å¥–åŠ±={result['reward']:7.1f} æœ€è¿œ={result['max_x_pos']:4d} è¿›åº¦={result['progress']:4d}"
            )

        # æ‰¾å‡ºæœ€å¥½çš„ç­–ç•¥
        best_result = max(results, key=lambda x: x["max_x_pos"])
        print(
            f"\nğŸ¯ æœ€ä½³ç­–ç•¥: {best_result['strategy']} (æœ€è¿œä½ç½®: {best_result['max_x_pos']})"
        )

        return results
    finally:
        env.close()


if __name__ == "__main__":
    print("=== Super Mario Bros A3C æ¨¡å‹æ¨ç†æµ‹è¯• ===")
    print("æµ‹è¯•ä¸åŒç­–ç•¥çš„è¡¨ç°...\n")
    run_episode_with_sampling()
