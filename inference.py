"""inference.py
æ¨ç†è„šæœ¬ - åŠ è½½è®­ç»ƒå¥½çš„ A3C æ¨¡å‹è¿›è¡Œ Super Mario Bros æ¸¸æˆæ¨ç†ã€‚

å¢å¼ºç‰¹æ€§ (2025-10-02 æ›´æ–°):
1. è‡ªé€‚åº” checkpoint ç›®å½•è§£æï¼šè‹¥ --checkpoint æŒ‡å‘ç›®å½•ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€æ–° (latest ä¼˜å…ˆå…¶åæŒ‰ global_update æ’åº)ã€‚
2. çµæ´» state_dict åŠ è½½ï¼šå…¼å®¹ä¿å­˜äºæœªç¼–è¯‘æ¨¡å‹æˆ– torch.compile å (å«/ä¸å« `_orig_mod.` å‰ç¼€) çš„æƒé‡ã€‚
3. å¯é€‰ä½¿ç”¨è®­ç»ƒæ—¶çš„å®Œæ•´ stage_schedule (--full-schedule)ï¼Œè€Œéå•å…³å¡å›ºå®šã€‚
4. æ€§èƒ½ç»Ÿè®¡ï¼šæ¯ä¸ª episode è¾“å‡º steps/secï¼Œå¹¶åœ¨æ±‡æ€»ä¸­æä¾›å¹³å‡æ¨ç†é€Ÿåº¦ã€‚
5. è¯¦ç»†ç¼ºå¤± / å¤šä½™æƒé‡é”®æç¤ºï¼ˆæˆªæ–­æ˜¾ç¤º + æ–‡ä»¶è½ç›˜ï¼‰ã€‚

Usage:
    python inference.py --checkpoint trained_models/run01/a3c_world1_stage1_latest.pt --episodes 5 --deterministic
    python inference.py --checkpoint trained_models/run01 --full-schedule --episodes 8
"""

import argparse
import json
import time
from pathlib import Path
from typing import Any, List, Tuple

import numpy as np
import torch
import torch.nn.functional as F
from torch.distributions import Categorical

from src.config import ModelConfig
from src.envs.mario import MarioEnvConfig, MarioVectorEnvConfig, create_vector_env
from src.models import MarioActorCritic

ISSUES_LOG_NAME = "inference_state_dict_issues.log"


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="æ¨ç† Super Mario Bros A3C æ¨¡å‹ (æ”¯æŒç›®å½•è‡ªåŠ¨è§£æ / å‰ç¼€è‡ªé€‚åº”åŠ è½½)"
    )
    parser.add_argument(
        "--checkpoint", type=str, required=True, help="æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ (.pt)"
    )
    parser.add_argument("--episodes", type=int, default=5, help="æ¨ç† episode æ•°é‡")
    parser.add_argument(
        "--render", action="store_true", help="æ˜¯å¦æ˜¾ç¤ºæ¸¸æˆç”»é¢ï¼ˆéœ€è¦æ˜¾ç¤ºå™¨ï¼‰"
    )
    parser.add_argument(
        "--deterministic",
        action="store_true",
        help="ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œï¼‰è€Œéé‡‡æ ·",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "cpu", "cuda"],
        help="æ¨ç†è®¾å¤‡",
    )
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument(
        "--max-steps", type=int, default=10000, help="å•ä¸ª episode æœ€å¤§æ­¥æ•°"
    )
    parser.add_argument(
        "--record-video",
        action="store_true",
        help="å½•åˆ¶æ¨ç†è§†é¢‘åˆ° output/inference_videos/",
    )
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡ºæ¯æ­¥ä¿¡æ¯")
    parser.add_argument(
        "--full-schedule",
        action="store_true",
        help="ä½¿ç”¨è®­ç»ƒ metadata ä¸­çš„å®Œæ•´ stage_schedule (å‘é‡ç¯å¢ƒæŒ‰é¡ºåºè·‘å¤šå…³å¡)",
    )
    parser.add_argument(
        "--max-missing-keys",
        type=int,
        default=20,
        help="æ§åˆ¶æ‰“å°ç¼ºå¤±/å¤šä½™æƒé‡é”®çš„æœ€å¤§æ•°é‡ (é»˜è®¤ 20)",
    )
    parser.add_argument(
        "--issues-log-dir",
        type=str,
        default=".",
        help="ä¿å­˜ state_dict åŠ è½½å·®å¼‚æ—¥å¿—çš„ç›®å½• (é»˜è®¤å½“å‰)",
    )
    return parser.parse_args()


def _candidate_checkpoints_in_dir(directory: Path) -> List[Path]:
    return sorted(directory.glob("a3c_world*_stage*.pt"))


def _select_best_checkpoint_in_dir(directory: Path) -> Path:
    candidates = _candidate_checkpoints_in_dir(directory)
    if not candidates:
        raise FileNotFoundError(f"ç›®å½•ä¸­æœªæ‰¾åˆ° *.pt: {directory}")
    # ä¼˜å…ˆ latestï¼Œå†æŒ‰ global_update è§£æ json å…ƒæ•°æ®æ’åº
    latest = [c for c in candidates if c.name.endswith("latest.pt")]
    if latest:
        return latest[0]
    scored: List[Tuple[int, int, Path]] = []  # (global_update, global_step, path)
    for ckpt in candidates:
        meta_p = ckpt.with_suffix(".json")
        if not meta_p.exists():
            continue
        try:
            meta = json.loads(meta_p.read_text(encoding="utf-8"))
            save_state = meta.get("save_state", {})
            upd = int(save_state.get("global_update", 0))
            step = int(save_state.get("global_step", 0))
            scored.append((upd, step, ckpt))
        except Exception:
            continue
    if not scored:
        return candidates[-1]
    scored.sort(reverse=True)
    return scored[0][2]


def resolve_checkpoint_path(arg_path: str) -> Path:
    p = Path(arg_path)
    if p.is_dir():
        return _select_best_checkpoint_in_dir(p)
    return p


def load_checkpoint_metadata(checkpoint_path: Path) -> dict:
    """åŠ è½½æ£€æŸ¥ç‚¹çš„å…ƒæ•°æ®é…ç½® (è¦æ±‚ä¸ .pt åŒå .json)ã€‚"""
    metadata_path = checkpoint_path.with_suffix(".json")
    if not metadata_path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹å…ƒæ•°æ®æ–‡ä»¶: {metadata_path}")
    return json.loads(metadata_path.read_text(encoding="utf-8"))


def create_inference_env(
    metadata: dict,
    render: bool = False,
    record_video: bool = False,
    full_schedule: bool = False,
):
    """æ ¹æ®æ¨¡å‹å…ƒæ•°æ®åˆ›å»ºæ¨ç†ç¯å¢ƒã€‚

    å‚æ•°:
        metadata: checkpoint json å†…å®¹ã€‚
        render: æ˜¯å¦å¼€å¯æ¸²æŸ“ã€‚
        record_video: æ˜¯å¦å½•åˆ¶è§†é¢‘ã€‚
        full_schedule: è‹¥ True ä¸” metadata.vector_env.stage_schedule å­˜åœ¨, ä½¿ç”¨å…¶åºåˆ—è€Œéå•å…³å¡ã€‚
    """
    schedule: List[Tuple[int, int]]
    if full_schedule:
        ve = metadata.get("vector_env", {})
        raw_sched = ve.get("stage_schedule") or []
        schedule = (
            [(int(w), int(s)) for (w, s) in raw_sched]
            if raw_sched
            else [(metadata["world"], metadata["stage"])]
        )
    else:
        schedule = [(metadata["world"], metadata["stage"])]

    env_cfg = MarioEnvConfig(
        world=metadata["world"],
        stage=metadata["stage"],
        action_type=metadata["action_type"],
        frame_skip=metadata["frame_skip"],
        frame_stack=metadata["frame_stack"],
        video_dir="output/inference_videos" if record_video else None,
        record_video=record_video,
    )
    vector_cfg = MarioVectorEnvConfig(
        num_envs=1,
        asynchronous=False,
        stage_schedule=tuple(schedule),
        random_start_stage=False,
        base_seed=42,
        env=env_cfg,
    )
    return create_vector_env(vector_cfg)


def _flex_load_state_dict(model: torch.nn.Module, state: dict) -> List[str]:
    """çµæ´»åŠ è½½ (å…¼å®¹ _orig_mod. å‰ç¼€); è¿”å›é—®é¢˜é”®åˆ—è¡¨ã€‚"""
    model_keys = set(model.state_dict().keys())
    state_keys = set(state.keys())
    model_prefixed = any(k.startswith("_orig_mod.") for k in model_keys)
    state_prefixed = any(k.startswith("_orig_mod.") for k in state_keys)
    adj = state
    if model_prefixed and not state_prefixed:
        adj = {f"_orig_mod.{k}": v for k, v in state.items()}
    elif not model_prefixed and state_prefixed:
        adj = {
            k[len("_orig_mod.") :]: v
            for k, v in state.items()
            if k.startswith("_orig_mod.")
        }
    missing, unexpected = model.load_state_dict(adj, strict=False)
    issues: List[str] = list(missing) + [f"unexpected:{u}" for u in unexpected]
    return issues


def load_model(
    checkpoint_path: Path,
    metadata: dict,
    device: torch.device,
    max_missing: int,
    issues_log_dir: Path,
) -> MarioActorCritic:
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ (å«è‡ªé€‚åº”é”®å‰ç¼€ + é—®é¢˜æ—¥å¿—)ã€‚"""
    model_cfg = ModelConfig(**metadata["model"])
    model = MarioActorCritic(model_cfg).to(device)
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    issues = _flex_load_state_dict(model, checkpoint["model"])
    model.eval()
    if issues:
        head = issues[:max_missing]
        more = (
            ""
            if len(issues) <= max_missing
            else f" ... (+{len(issues)-max_missing} more)"
        )
        print(f"[inference][warn] state_dict å·®å¼‚: {len(issues)} -> {head}{more}")
        try:
            issues_log_dir.mkdir(parents=True, exist_ok=True)
            (issues_log_dir / ISSUES_LOG_NAME).write_text(
                "\n".join(issues), encoding="utf-8"
            )
            print(
                f"[inference] å·®å¼‚å·²å†™å…¥: {(issues_log_dir / ISSUES_LOG_NAME).as_posix()}"
            )
        except Exception as e:  # pragma: no cover
            print(f"[inference][warn] å†™å…¥å·®å¼‚æ—¥å¿—å¤±è´¥: {e}")
    print(f"[inference] æˆåŠŸåŠ è½½æ¨¡å‹: {checkpoint_path}")
    print(f"[inference] æ¨¡å‹é…ç½®: {model_cfg}")
    if "global_step" in checkpoint:
        print(f"[inference] è®­ç»ƒæ­¥æ•°: {checkpoint['global_step']}")
    if "global_update" in checkpoint:
        print(f"[inference] è®­ç»ƒè½®æ¬¡: {checkpoint['global_update']}")
    return model


def _extract_single_info(raw_info: Any) -> dict:
    """Normalize vector env info (list[dict] | dict | other) -> dict.

    fc_emulator backend + SyncVectorEnv ä¸‹å¯èƒ½è¿”å›å•ä¸ª dict (é list)ã€‚
    å…¼å®¹å¤„ç†ï¼š
      - list ä¸”é¦–å…ƒç´ ä¸º dict -> è¿”å›é¦–å…ƒç´ 
      - dict -> ç›´æ¥è¿”å›
      - å…¶å®ƒ -> {}
    """
    if isinstance(raw_info, list):
        if raw_info and isinstance(raw_info[0], dict):
            return raw_info[0]
        return {}
    if isinstance(raw_info, dict):
        return raw_info
    return {}


def _coerce_scalar(x):  # noqa: D401 - simple helper
    """å°† numpy æ ‡é‡ / ndarray[0] / tensor è½¬ä¸º Python æ ‡é‡ int/floatã€‚"""
    try:
        import numpy as _np  # local
        import torch as _t

        if isinstance(x, _t.Tensor):
            if x.numel() == 1:
                return x.item()
            return x
        if isinstance(x, _np.ndarray):
            if x.size == 1:
                return x.flatten()[0].item()
            return x
        if hasattr(x, "item"):
            return x.item()
    except Exception:
        pass
    return x


def run_inference_episode(
    env,
    model: MarioActorCritic,
    device: torch.device,
    max_steps: int = 10000,
    deterministic: bool = False,
    verbose: bool = False,
) -> tuple[float, int, dict, float]:
    """è¿è¡Œå•ä¸ªæ¨ç† episode"""
    obs_np, info = env.reset()
    obs = torch.as_tensor(obs_np, dtype=torch.float32, device=device)

    # åˆå§‹åŒ–éšè—çŠ¶æ€
    hidden_state, cell_state = model.initial_state(1, device)

    total_reward = 0.0
    steps = 0
    episode_info = {"x_pos": [], "stage": [], "flag_get": False, "life": 2}

    start_time = time.time()
    for step in range(max_steps):
        with torch.no_grad():
            output = model(obs, hidden_state, cell_state)
            logits = output.logits
            value = output.value

            if deterministic:
                # ç¡®å®šæ€§ç­–ç•¥ï¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œ
                action = torch.argmax(logits, dim=-1)
            else:
                # éšæœºç­–ç•¥ï¼šæŒ‰æ¦‚ç‡é‡‡æ ·
                dist = Categorical(logits=logits)
                action = dist.sample()

        action_np = action.cpu().numpy()
        obs_np, reward_np, terminated, truncated, raw_info = env.step(action_np)
        info_item = _extract_single_info(raw_info)
        # å…¼å®¹ reward_np shape=[1] æˆ–æ ‡é‡
        try:
            reward = float(reward_np[0] if hasattr(reward_np, "__len__") else reward_np)
        except Exception:
            reward = float(reward_np)
        total_reward += reward
        steps += 1

        # è®°å½•æ¸¸æˆä¿¡æ¯
        if info_item:
            # å…¼å®¹ fc_emulator metrics
            x_pos_val = info_item.get("x_pos")
            if x_pos_val is None:
                metrics = info_item.get("metrics")
                if isinstance(metrics, dict):
                    x_pos_val = metrics.get("mario_x") or metrics.get("x_pos")
            if x_pos_val is not None:
                x_pos_val = _coerce_scalar(x_pos_val)
                try:
                    episode_info["x_pos"].append(int(x_pos_val))
                except Exception:
                    pass
            episode_info["stage"].append(info_item.get("stage", 1))
            if info_item.get("flag_get", False):
                episode_info["flag_get"] = True
            if "life" in info_item:
                try:
                    episode_info["life"] = int(_coerce_scalar(info_item["life"]))
                except Exception:
                    pass

        if verbose and (step + 1) % 100 == 0:
            action_id = int(action_np[0])
            prob = F.softmax(logits, dim=-1)[0, action_id].item()
            print(
                f"  æ­¥æ•° {step+1:4d}: åŠ¨ä½œ={action_id:2d} æ¦‚ç‡={prob:.3f} å¥–åŠ±={reward:6.1f} ä»·å€¼={value.item():6.2f}"
            )

        # æ›´æ–°è§‚æµ‹å’Œéšè—çŠ¶æ€
        obs = torch.as_tensor(obs_np, dtype=torch.float32, device=device)
        if output.hidden_state is not None:
            hidden_state = output.hidden_state
            if terminated[0] or truncated[0]:
                hidden_state = hidden_state * 0.0  # é‡ç½®éšè—çŠ¶æ€
            if output.cell_state is not None:
                cell_state = output.cell_state
                if terminated[0] or truncated[0]:
                    cell_state = cell_state * 0.0

        # æ£€æŸ¥ episode ç»“æŸ
        # terminated / truncated å…¼å®¹æ ‡é‡æˆ– ndarray
        term_flag = terminated[0] if hasattr(terminated, "__len__") else terminated
        trunc_flag = truncated[0] if hasattr(truncated, "__len__") else truncated
        if term_flag or trunc_flag:
            break
    duration = time.time() - start_time
    return total_reward, steps, episode_info, duration


def main():
    args = parse_args()

    # è®¾å¤‡é…ç½®
    if args.device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)
    print(f"[inference] ä½¿ç”¨è®¾å¤‡: {device}")

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    # åŠ è½½æ£€æŸ¥ç‚¹å’Œå…ƒæ•°æ®
    checkpoint_path = resolve_checkpoint_path(args.checkpoint)
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")

    print(f"[inference] åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    metadata = load_checkpoint_metadata(checkpoint_path)

    # åˆ›å»ºç¯å¢ƒ
    print(
        f"[inference] åˆ›å»ºæ¨ç†ç¯å¢ƒ World {metadata['world']}-{metadata['stage']} ({metadata['action_type']})"
    )
    env = create_inference_env(
        metadata,
        render=args.render,
        record_video=args.record_video,
        full_schedule=args.full_schedule,
    )

    try:
        # åŠ è½½æ¨¡å‹
        model = load_model(
            checkpoint_path,
            metadata,
            device,
            max_missing=args.max_missing_keys,
            issues_log_dir=Path(args.issues_log_dir),
        )

        # è¿è¡Œæ¨ç†
        print(f"[inference] å¼€å§‹æ¨ç† {args.episodes} ä¸ª episode...")
        print("=" * 80)

        results = []
        total_start_time = time.time()
        total_env_steps = 0
        total_env_time = 0.0
        for episode in range(args.episodes):
            print(f"Episode {episode + 1}/{args.episodes}")

            episode_start_time = time.time()
            reward, steps, info, epi_time = run_inference_episode(
                env,
                model,
                device,
                max_steps=args.max_steps,
                deterministic=args.deterministic,
                verbose=args.verbose,
            )
            episode_duration = time.time() - episode_start_time
            steps_per_sec = steps / epi_time if epi_time > 0 else 0.0
            total_env_steps += steps
            total_env_time += epi_time

            results.append(
                {
                    "episode": episode + 1,
                    "reward": reward,
                    "steps": steps,
                    "duration": episode_duration,
                    "flag_get": info["flag_get"],
                    "max_x_pos": max(info["x_pos"]) if info["x_pos"] else 0,
                    "final_life": info["life"],
                }
            )

            status = (
                "ğŸ é€šå…³!"
                if info["flag_get"]
                else f"âŒ å¤±è´¥ (æœ€è¿œä½ç½®: {max(info['x_pos']) if info['x_pos'] else 0})"
            )
            print(f"  ç»“æœ: {status}")
            print(
                f"  å¥–åŠ±: {reward:8.1f} | æ­¥æ•°: {steps:5d} | æ—¶é•¿: {episode_duration:6.2f}s | é€Ÿç‡: {steps_per_sec:6.1f} steps/s"
            )
            print("-" * 80)

        # æ±‡æ€»ç»Ÿè®¡
        total_duration = time.time() - total_start_time
        avg_reward = np.mean([r["reward"] for r in results])
        avg_steps = np.mean([r["steps"] for r in results])
        success_rate = np.mean([r["flag_get"] for r in results]) * 100
        max_reward = max([r["reward"] for r in results])

        print("\nğŸ“Š æ¨ç†æ±‡æ€»ç»Ÿè®¡:")
        print(
            f"  æˆåŠŸç‡: {success_rate:5.1f}% ({sum(r['flag_get'] for r in results)}/{args.episodes})"
        )
        print(f"  å¹³å‡å¥–åŠ±: {avg_reward:8.1f} (æœ€é«˜: {max_reward:8.1f})")
        print(f"  å¹³å‡æ­¥æ•°: {avg_steps:8.1f}")
        print(f"  æ€»è€—æ—¶: {total_duration:6.2f}s")
        if total_env_time > 0:
            print(
                f"  å¹³å‡æ¨ç†é€Ÿç‡: {total_env_steps/total_env_time:6.1f} steps/s (åŸºäºæ— æ¸²æŸ“è®¡æ—¶)"
            )
        print(f"  ç­–ç•¥æ¨¡å¼: {'ç¡®å®šæ€§' if args.deterministic else 'éšæœºé‡‡æ ·'}")

        if args.record_video:
            print("  è§†é¢‘å·²ä¿å­˜è‡³: output/inference_videos/")
        print("[inference] æ¨ç†å®Œæˆ")

    finally:
        env.close()


if __name__ == "__main__":
    main()
